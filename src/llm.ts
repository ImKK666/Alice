// src/llm.ts
/**
 * LLM æ¨¡å‹å®¢æˆ·ç«¯æ¨¡å— - æä¾›å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨åŠŸèƒ½
 *
 * å®ç°åŠŸèƒ½ï¼š
 * 1. ä½¿ç”¨ DeepSeek API ç”Ÿæˆæ–‡æœ¬å“åº”
 * 2. é…ç½®æ¨¡å‹å‚æ•°å¦‚æ¸©åº¦ã€æœ€å¤§æ ‡è®°æ•°ç­‰
 * 3. å¤„ç† API è°ƒç”¨é”™è¯¯å’Œé‡è¯•
 * 4. å¼‚æ­¥LLMè¯·æ±‚ç®¡ç†å’Œä¼˜åŒ–
 */
import { ChatOpenAI } from "@langchain/openai"; // ä½¿ç”¨ OpenAI å…¼å®¹çš„ API æ ¼å¼
import { config } from "./config.ts";
import { createModuleLogger } from "./utils/logger.ts";
import { PerformanceMonitor } from "./utils/performance.ts";

// æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§
const llmLogger = createModuleLogger("LLM");
const performanceMonitor = PerformanceMonitor.getInstance();

/**
 * åˆ›å»º LLM å®¢æˆ·ç«¯å®ä¾‹
 *
 * ä½¿ç”¨ ChatOpenAI ç±»ä½œä¸ºå®¢æˆ·ç«¯ï¼Œå› ä¸º DeepSeek æä¾›äº†ä¸ OpenAI å…¼å®¹çš„ API
 * è¿™é‡Œé…ç½®äº†å„ç§å‚æ•°æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹
 */
export const llm = new ChatOpenAI({
  // æ¨¡å‹é…ç½®
  modelName: config.llmModel, // æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼Œä»é…ç½®è¯»å–

  // ç”Ÿæˆå‚æ•°
  temperature: 0.75, // æ¸©åº¦è°ƒæ•´ - ç•¥å¾®æé«˜ä»¥å¢åŠ è‡ªç„¶åº¦ (åŸä¸º0.7)
  maxTokens: 4096, // é™åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦ (åŸä¸º65536ï¼Œå¯èƒ½è¿‡é«˜)
  // æ³¨æ„: Deepseekæ¨¡å‹çš„å®é™…ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶å¯èƒ½ä¸åŒ
  // éœ€è¦æ ¹æ®æ‰€é€‰æ¨¡å‹çš„æ–‡æ¡£è°ƒæ•´

  // èº«ä»½éªŒè¯
  apiKey: config.deepseekApiKey, // ä½¿ç”¨DeepSeek APIå¯†é’¥

  // API ç«¯ç‚¹é…ç½®
  configuration: {
    baseURL: config.deepseekBaseUrl, // ä½¿ç”¨DeepSeek APIåŸºç¡€URL
  },

  // é”™è¯¯å¤„ç†
  maxRetries: 3, // ç¨å¾®å¢åŠ é‡è¯•æ¬¡æ•°
  timeout: 120000, // è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º120ç§’ (2åˆ†é’Ÿ)ï¼Œé˜²æ­¢è¯·æ±‚å¡æ­»
  // é«˜çº§åŠŸèƒ½ï¼ˆå½“å‰æœªå¯ç”¨ï¼‰
  // streaming: true, // æµå¼å“åº” - å¦‚æœéœ€è¦å®æ—¶è·å–ç”Ÿæˆç»“æœï¼Œå¯ä»¥å¼€å¯
});

// --- å¼‚æ­¥LLMç®¡ç†å™¨ ---

/**
 * LLMè¯·æ±‚ç±»å‹æšä¸¾
 */
export enum LLMRequestType {
  MESSAGE_ANALYSIS = "message_analysis",
  SENTIMENT_ANALYSIS = "sentiment_analysis",
  RESPONSE_GENERATION = "response_generation",
  HUMANIZATION = "humanization",
  SOCIAL_ANALYSIS = "social_analysis",
  IMPORTANCE_DETECTION = "importance_detection",
  MIND_WANDERING = "mind_wandering",
  EMBODIED_EXPRESSION = "embodied_expression",
  OTHER = "other"
}

/**
 * LLMè¯·æ±‚ä¼˜å…ˆçº§
 */
export enum LLMRequestPriority {
  CRITICAL = 1,    // å…³é”®è·¯å¾„ï¼Œå¿…é¡»ç«‹å³å¤„ç†
  HIGH = 2,        // é«˜ä¼˜å…ˆçº§ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ
  NORMAL = 3,      // æ­£å¸¸ä¼˜å…ˆçº§
  LOW = 4,         // ä½ä¼˜å…ˆçº§ï¼Œå¯å»¶è¿Ÿå¤„ç†
  BACKGROUND = 5   // åå°ä»»åŠ¡
}

/**
 * LLMè¯·æ±‚æ¥å£
 */
interface LLMRequest {
  id: string;
  type: LLMRequestType;
  priority: LLMRequestPriority;
  prompt: string;
  options?: {
    temperature?: number;
    maxTokens?: number;
    timeout?: number;
  };
  timestamp: number;
  userId?: string;
  contextId?: string;
}

/**
 * LLMå“åº”æ¥å£
 */
interface LLMResponse {
  requestId: string;
  content: string;
  success: boolean;
  error?: Error;
  duration: number;
  timestamp: number;
}

/**
 * å¼‚æ­¥LLMè¯·æ±‚ç®¡ç†å™¨
 */
class AsyncLLMManager {
  private requestQueue: Map<string, LLMRequest> = new Map();
  private responseCache: Map<string, LLMResponse> = new Map();
  private activeRequests: Set<string> = new Set();
  private maxConcurrentRequests = 5; // æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
  private cacheTimeout = 5 * 60 * 1000; // ç¼“å­˜5åˆ†é’Ÿ
  private requestDeduplication: Map<string, string[]> = new Map(); // è¯·æ±‚å»é‡

  /**
   * æäº¤LLMè¯·æ±‚
   */
  async submitRequest(
    type: LLMRequestType,
    priority: LLMRequestPriority,
    prompt: string,
    options?: {
      temperature?: number;
      maxTokens?: number;
      timeout?: number;
      userId?: string;
      contextId?: string;
    }
  ): Promise<string> {
    const requestId = crypto.randomUUID();
    const operationId = `llm_request_${type}_${Date.now()}`;

    // æ£€æŸ¥ç¼“å­˜å’Œå»é‡
    const cacheKey = this.generateCacheKey(type, prompt, options);
    const cachedResponse = this.getCachedResponse(cacheKey);
    if (cachedResponse) {
      llmLogger.debug(`ç¼“å­˜å‘½ä¸­: ${type}`, { requestId, cacheKey });
      return cachedResponse.content;
    }

    // æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„è¯·æ±‚æ­£åœ¨å¤„ç†
    const duplicateRequestId = this.checkDuplicateRequest(cacheKey);
    if (duplicateRequestId) {
      llmLogger.debug(`è¯·æ±‚å»é‡: ${type}`, { requestId, duplicateRequestId });
      return this.waitForDuplicateRequest(duplicateRequestId);
    }

    const request: LLMRequest = {
      id: requestId,
      type,
      priority,
      prompt,
      options,
      timestamp: Date.now(),
      userId: options?.userId,
      contextId: options?.contextId
    };

    performanceMonitor.startOperation(operationId, `LLMè¯·æ±‚-${type}`, options?.userId);

    try {
      // æ·»åŠ åˆ°å»é‡æ˜ å°„
      this.addToDeduplication(cacheKey, requestId);

      // æ‰§è¡Œè¯·æ±‚
      const response = await this.executeRequest(request);

      // ç¼“å­˜å“åº”
      this.cacheResponse(cacheKey, response);

      performanceMonitor.endOperation(operationId, `LLMè¯·æ±‚-${type}`, options?.userId);
      llmLogger.info(`LLMè¯·æ±‚å®Œæˆ: ${type}`, {
        requestId,
        duration: response.duration,
        success: response.success
      });

      if (!response.success) {
        throw response.error || new Error("LLMè¯·æ±‚å¤±è´¥");
      }

      return response.content;
    } catch (error) {
      performanceMonitor.endOperation(operationId, `LLMè¯·æ±‚-${type}`, options?.userId);
      llmLogger.error(`LLMè¯·æ±‚å¤±è´¥: ${type}`, error as Error, { requestId });
      throw error;
    } finally {
      // æ¸…ç†å»é‡æ˜ å°„
      this.removeFromDeduplication(cacheKey, requestId);
    }
  }

  /**
   * æ‰§è¡Œå•ä¸ªLLMè¯·æ±‚
   */
  private async executeRequest(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    this.activeRequests.add(request.id);

    try {
      const timeout = request.options?.timeout || 60000; // é»˜è®¤60ç§’è¶…æ—¶
      const timeoutPromise = new Promise<never>((_, reject) => {
        setTimeout(() => reject(new Error("LLMè¯·æ±‚è¶…æ—¶")), timeout);
      });

      const llmPromise = llm.invoke(request.prompt, {
        temperature: request.options?.temperature || 0.75,
        maxTokens: request.options?.maxTokens || 4096,
      });

      const response = await Promise.race([llmPromise, timeoutPromise]);
      const content = typeof response === "string" ? response : (response.content as string);

      return {
        requestId: request.id,
        content: content || "",
        success: true,
        duration: Date.now() - startTime,
        timestamp: Date.now()
      };
    } catch (error) {
      return {
        requestId: request.id,
        content: "",
        success: false,
        error: error as Error,
        duration: Date.now() - startTime,
        timestamp: Date.now()
      };
    } finally {
      this.activeRequests.delete(request.id);
    }
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(
    type: LLMRequestType,
    prompt: string,
    options?: any
  ): string {
    const optionsStr = options ? JSON.stringify(options) : "";
    const hash = this.simpleHash(prompt + optionsStr);
    return `${type}_${hash}`;
  }

  /**
   * ç®€å•å“ˆå¸Œå‡½æ•°
   */
  private simpleHash(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return Math.abs(hash).toString(36);
  }

  /**
   * è·å–ç¼“å­˜å“åº”
   */
  private getCachedResponse(cacheKey: string): LLMResponse | null {
    const cached = this.responseCache.get(cacheKey);
    if (cached && (Date.now() - cached.timestamp) < this.cacheTimeout) {
      return cached;
    }
    if (cached) {
      this.responseCache.delete(cacheKey); // æ¸…ç†è¿‡æœŸç¼“å­˜
    }
    return null;
  }

  /**
   * ç¼“å­˜å“åº”
   */
  private cacheResponse(cacheKey: string, response: LLMResponse): void {
    if (response.success) {
      this.responseCache.set(cacheKey, response);
    }
  }

  /**
   * æ£€æŸ¥é‡å¤è¯·æ±‚
   */
  private checkDuplicateRequest(cacheKey: string): string | null {
    const requestIds = this.requestDeduplication.get(cacheKey);
    return requestIds && requestIds.length > 0 ? requestIds[0] : null;
  }

  /**
   * æ·»åŠ åˆ°å»é‡æ˜ å°„
   */
  private addToDeduplication(cacheKey: string, requestId: string): void {
    const existing = this.requestDeduplication.get(cacheKey) || [];
    existing.push(requestId);
    this.requestDeduplication.set(cacheKey, existing);
  }

  /**
   * ä»å»é‡æ˜ å°„ä¸­ç§»é™¤
   */
  private removeFromDeduplication(cacheKey: string, requestId: string): void {
    const existing = this.requestDeduplication.get(cacheKey) || [];
    const filtered = existing.filter(id => id !== requestId);
    if (filtered.length === 0) {
      this.requestDeduplication.delete(cacheKey);
    } else {
      this.requestDeduplication.set(cacheKey, filtered);
    }
  }

  /**
   * ç­‰å¾…é‡å¤è¯·æ±‚å®Œæˆ
   */
  private async waitForDuplicateRequest(requestId: string): Promise<string> {
    // ç®€å•çš„è½®è¯¢ç­‰å¾…ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨äº‹ä»¶æœºåˆ¶
    let attempts = 0;
    const maxAttempts = 60; // æœ€å¤šç­‰å¾…60ç§’

    while (attempts < maxAttempts) {
      if (!this.activeRequests.has(requestId)) {
        // è¯·æ±‚å·²å®Œæˆï¼ŒæŸ¥æ‰¾ç¼“å­˜ç»“æœ
        for (const [cacheKey, response] of this.responseCache.entries()) {
          if (response.requestId === requestId && response.success) {
            return response.content;
          }
        }
        break;
      }
      await new Promise(resolve => setTimeout(resolve, 1000));
      attempts++;
    }

    throw new Error("ç­‰å¾…é‡å¤è¯·æ±‚è¶…æ—¶");
  }

  /**
   * æ‰¹é‡æäº¤è¯·æ±‚
   */
  async submitBatchRequests(
    requests: Array<{
      type: LLMRequestType;
      priority: LLMRequestPriority;
      prompt: string;
      options?: any;
    }>
  ): Promise<string[]> {
    const promises = requests.map(req =>
      this.submitRequest(req.type, req.priority, req.prompt, req.options)
    );

    const results = await Promise.allSettled(promises);
    return results.map(result =>
      result.status === 'fulfilled' ? result.value : ""
    );
  }

  /**
   * è·å–ç®¡ç†å™¨çŠ¶æ€
   */
  getStatus() {
    return {
      activeRequests: this.activeRequests.size,
      cacheSize: this.responseCache.size,
      deduplicationSize: this.requestDeduplication.size,
      maxConcurrentRequests: this.maxConcurrentRequests
    };
  }

  /**
   * æ¸…ç†è¿‡æœŸç¼“å­˜
   */
  cleanupCache(): void {
    const now = Date.now();
    for (const [key, response] of this.responseCache.entries()) {
      if (now - response.timestamp > this.cacheTimeout) {
        this.responseCache.delete(key);
      }
    }
  }
}

// åˆ›å»ºå…¨å±€LLMç®¡ç†å™¨å®ä¾‹
export const asyncLLMManager = new AsyncLLMManager();

// å®šæœŸæ¸…ç†ç¼“å­˜
setInterval(() => {
  asyncLLMManager.cleanupCache();
}, 5 * 60 * 1000); // æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

/**
 * ä¾¿æ·çš„LLMè°ƒç”¨å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
 */
export async function invokeLLM(
  prompt: string,
  type: LLMRequestType = LLMRequestType.OTHER,
  priority: LLMRequestPriority = LLMRequestPriority.NORMAL,
  options?: {
    temperature?: number;
    maxTokens?: number;
    timeout?: number;
    userId?: string;
    contextId?: string;
  }
): Promise<string> {
  return asyncLLMManager.submitRequest(type, priority, prompt, options);
}

/**
 * è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯
 *
 * åœ¨åˆå§‹åŒ– LLM å®¢æˆ·ç«¯åè¾“å‡ºæ—¥å¿—ï¼Œä¾¿äºè°ƒè¯•å’Œç¡®è®¤
 */
console.log(
  `ğŸ§  å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆã€‚æ¨¡å‹: ${config.llmModel}, APIç«¯ç‚¹: ${config.deepseekBaseUrl}`,
);
console.log(`ğŸš€ å¼‚æ­¥LLMç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œæ”¯æŒå¹¶å‘è¯·æ±‚ã€ç¼“å­˜å’Œå»é‡ä¼˜åŒ–`);
